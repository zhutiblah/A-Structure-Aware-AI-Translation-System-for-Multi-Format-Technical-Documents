# latex_processor.py (å®Œæ•´ç‰ˆ - æ·»åŠ ç¼“å­˜æ¸…ç† + å®Œæ•´æ–‡ä»¶å¤åˆ¶)
import os
import re
import logging
from typing import List, Dict
import shutil
import requests
import json
import time
import hashlib
from datetime import datetime, timedelta
from latex_translation import translate_cls_or_sty_file, ClsStyTranslator
CLS_TRANSLATOR_AVAILABLE = True

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class TranslationCache:
    """ç¿»è¯‘ç¼“å­˜ç®¡ç†å™¨ï¼ˆå¢å¼ºç‰ˆ - å¸¦æ¸…ç†æœºåˆ¶ï¼‰"""
    
    def __init__(self, cache_file='translation_cache.json', max_age_days=30, max_entries=10000):
        self.cache_file = cache_file
        self.max_age_days = max_age_days
        self.max_entries = max_entries
        self.cache = self._load_cache()
        self.hits = 0
        self.misses = 0
        
        # åŠ è½½åç«‹å³æ¸…ç†
        self._cleanup_cache()
    
    def _load_cache(self):
        """åŠ è½½ç¼“å­˜æ–‡ä»¶ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
        if not os.path.exists(self.cache_file):
            return {}
        
        try:
            with open(self.cache_file, 'r', encoding='utf-8') as f:
                cache_data = json.load(f)
                
            if not isinstance(cache_data, dict):
                logger.warning("Invalid cache format, creating new cache")
                return {}
                
            # è¿‡æ»¤æ‰è¿‡å¤§çš„æ¡ç›®
            filtered_cache = {}
            removed_count = 0
            for key, value in cache_data.items():
                if isinstance(value, dict):
                    text = value.get('text', '')
                    if len(text) < 50000:
                        filtered_cache[key] = value
                    else:
                        removed_count += 1
                elif isinstance(value, str):
                    if len(value) < 50000:
                        filtered_cache[key] = {
                            'text': value,
                            'timestamp': datetime.now().isoformat()
                        }
                    else:
                        removed_count += 1
                
            if removed_count > 0:
                logger.info(f"â™»ï¸ Removed {removed_count} oversized cache entries")
                
            return filtered_cache
            
        except json.JSONDecodeError as e:
            logger.warning(f"Cache file corrupted: {e}, creating new cache")
            backup_file = self.cache_file + f'.backup.{int(time.time())}'
            try:
                shutil.copy2(self.cache_file, backup_file)
                logger.info(f"Corrupted cache backed up to {backup_file}")
            except:
                pass
            return {}
            
        except Exception as e:
            logger.warning(f"Failed to load cache: {e}")
            return {}
    
    def _cleanup_cache(self):
        """æ¸…ç†è¿‡æœŸå’Œè¿‡å¤šçš„ç¼“å­˜"""
        if not self.cache:
            return
        
        original_size = len(self.cache)
        cutoff_date = datetime.now() - timedelta(days=self.max_age_days)
        
        # 1. åˆ é™¤è¿‡æœŸæ¡ç›®
        expired_keys = []
        for key, value in self.cache.items():
            if isinstance(value, dict):
                try:
                    timestamp = datetime.fromisoformat(value.get('timestamp', ''))
                    if timestamp < cutoff_date:
                        expired_keys.append(key)
                except:
                    pass
        
        for key in expired_keys:
            del self.cache[key]
        
        if expired_keys:
            logger.info(f"â™»ï¸ Removed {len(expired_keys)} expired cache entries (>{self.max_age_days} days)")
        
        # 2. å¦‚æœä»è¶…è¿‡æœ€å¤§æ¡ç›®æ•°ï¼Œåˆ é™¤æœ€æ—§çš„
        if len(self.cache) > self.max_entries:
            sorted_items = sorted(
                self.cache.items(),
                key=lambda x: x[1].get('timestamp', '') if isinstance(x[1], dict) else '',
                reverse=False
            )
            
            keep_count = self.max_entries
            items_to_remove = sorted_items[:-keep_count] if keep_count > 0 else sorted_items
            
            for key, _ in items_to_remove:
                del self.cache[key]
            
            logger.info(f"â™»ï¸ Removed {len(items_to_remove)} oldest entries (limit: {self.max_entries})")
        
        cleaned_count = original_size - len(self.cache)
        if cleaned_count > 0:
            logger.info(f"ğŸ“Š Cache cleanup: {original_size} â†’ {len(self.cache)} entries ({cleaned_count} removed)")
            self._save_cache()
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜ï¼ˆå¸¦é”™è¯¯å¤„ç†ï¼‰"""
        try:
            temp_file = self.cache_file + '.tmp'
            with open(temp_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
            
            with open(temp_file, 'r', encoding='utf-8') as f:
                json.load(f)
            
            if os.path.exists(self.cache_file):
                os.replace(temp_file, self.cache_file)
            else:
                os.rename(temp_file, self.cache_file)
            
        except Exception as e:
            logger.warning(f"Failed to save cache: {e}")
            if os.path.exists(temp_file):
                try:
                    os.remove(temp_file)
                except:
                    pass
    
    def get_cache_key(self, text: str, model: str, direction: str) -> str:
        """ç”Ÿæˆç¼“å­˜é”®"""
        content = f"{text}|{model}|{direction}"
        return hashlib.md5(content.encode('utf-8')).hexdigest()
    
    def get(self, text: str, model: str, direction: str) -> str:
        """è·å–ç¼“å­˜"""
        try:
            key = self.get_cache_key(text, model, direction)
            if key in self.cache:
                self.hits += 1
                value = self.cache[key]
                if isinstance(value, dict):
                    return value.get('text', '')
                return value
            self.misses += 1
            return None
        except Exception as e:
            logger.warning(f"Cache get failed: {e}")
            self.misses += 1
            return None
    
    def set(self, text: str, model: str, direction: str, translation: str):
        """ä¿å­˜ç¼“å­˜ï¼ˆå¸¦éªŒè¯å’Œæ—¶é—´æˆ³ï¼‰"""
        try:
            if len(text) > 20000 or len(translation) > 20000:
                logger.warning(f"âš ï¸ Entry too large (text:{len(text)}, trans:{len(translation)}), skipping cache")
                return
            
            key = self.get_cache_key(text, model, direction)
            self.cache[key] = {
                'text': translation,
                'timestamp': datetime.now().isoformat()
            }
            
            if (self.hits + self.misses) % 10 == 0:
                self._save_cache()
            
        except Exception as e:
            logger.warning(f"Cache set failed: {e}")
    
    def clear_all(self):
        """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
        self.cache.clear()
        self._save_cache()
        logger.info("ğŸ—‘ï¸ All cache cleared")
    
    def clear_old(self, days: int = None):
        """æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„ç¼“å­˜"""
        if days is None:
            days = self.max_age_days
        
        original_size = len(self.cache)
        cutoff_date = datetime.now() - timedelta(days=days)
        
        keys_to_remove = []
        for key, value in self.cache.items():
            if isinstance(value, dict):
                try:
                    timestamp = datetime.fromisoformat(value.get('timestamp', ''))
                    if timestamp < cutoff_date:
                        keys_to_remove.append(key)
                except:
                    pass
        
        for key in keys_to_remove:
            del self.cache[key]
        
        removed = len(keys_to_remove)
        if removed > 0:
            self._save_cache()
            logger.info(f"ğŸ—‘ï¸ Cleared {removed} cache entries older than {days} days")
        else:
            logger.info(f"âœ… No cache entries older than {days} days")
    
    def get_stats(self) -> Dict:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        total = self.hits + self.misses
        hit_rate = self.hits / total * 100 if total > 0 else 0
        
        cache_size_bytes = 0
        if os.path.exists(self.cache_file):
            cache_size_bytes = os.path.getsize(self.cache_file)
        cache_size_mb = cache_size_bytes / (1024 * 1024)
        
        return {
            'hits': self.hits,
            'misses': self.misses,
            'total': total,
            'hit_rate': f"{hit_rate:.1f}%",
            'cache_entries': len(self.cache),
            'cache_size_mb': f"{cache_size_mb:.2f} MB"
        }
    
    def close(self):
        """å…³é—­å¹¶ä¿å­˜"""
        try:
            self._save_cache()
            stats = self.get_stats()
            logger.info(f"ğŸ“Š Cache Stats: {stats['hits']} hits, {stats['misses']} misses, "
                       f"hit rate {stats['hit_rate']}, {stats['cache_entries']} entries, "
                       f"size {stats['cache_size_mb']}")
        except Exception as e:
            logger.warning(f"Failed to close cache: {e}")


class ClsStyTranslator:
    """LaTeX æ–‡æ¡£ç¿»è¯‘å™¨ï¼ˆç®€åŒ–ç‰ˆ - é€‚é… LLMï¼‰"""
    
    # ğŸ”§ ç®€åŒ–ä¿æŠ¤æ¨¡å¼ï¼šåªä¿æŠ¤å…³é”®çš„æ–‡æ¡£ç»“æ„å’Œå¼•ç”¨ç³»ç»Ÿ
    PROTECTED_PATTERNS = [
        # === æ–‡æ¡£ç»“æ„ï¼ˆå¿…é¡»ä¿æŠ¤ï¼‰===
        (r'\\documentclass(\[.*?\])?\{.*?\}', 'DOCUMENTCLASS'),
        (r'\\usepackage(\[.*?\])?\{.*?\}', 'USEPACKAGE'),
        (r'\\begin\{document\}', 'BEGINDOC'),
        (r'\\end\{document\}', 'ENDDOC'),
        
        # === æ–‡ä»¶å¼•ç”¨ï¼ˆå¿…é¡»ä¿æŠ¤ï¼‰===
        (r'\\input\{.*?\}', 'INPUT'),
        (r'\\include\{.*?\}', 'INCLUDE'),
        
        # === å¼•ç”¨ç³»ç»Ÿï¼ˆå¿…é¡»ä¿æŠ¤ï¼‰===
        (r'\\cite(\[.*?\])?(\[.*?\])?\{.*?\}', 'CITE'),
        (r'\\parencite(\[.*?\])?(\[.*?\])?\{.*?\}', 'PARENCITE'),
        (r'\\ref\{.*?\}', 'REF'),
        (r'\\label\{.*?\}', 'LABEL'),
        
        # === å‚è€ƒæ–‡çŒ®ï¼ˆå¿…é¡»ä¿æŠ¤ï¼‰===
        (r'\\bibliography\{.*?\}', 'BIBLIO'),
        (r'\\bibliographystyle\{.*?\}', 'BIBLIOSTYLE'),
        
        # === è‡ªå®šä¹‰å‘½ä»¤ï¼ˆå¿…é¡»ä¿æŠ¤ï¼‰===
        (r'\\newcommand\{.*?\}(\[.*?\])?\{.*?\}', 'NEWCOMMAND'),
        (r'\\renewcommand\{.*?\}(\[.*?\])?\{.*?\}', 'RENEWCOMMAND')
    ]
    def __init__(self):
        self.placeholder_map = {}
        self.placeholder_counter = 0
    
    def _generate_placeholder(self, prefix: str) -> str:
        self.placeholder_counter += 1
        return f"<{prefix}_{self.placeholder_counter}>"
    
    def protect_latex_commands(self, text: str) -> str:
        """ä¿æŠ¤ LaTeX å‘½ä»¤"""
        protected_text = text
        
        for pattern, prefix in self.PROTECTED_PATTERNS:
            matches = re.finditer(pattern, protected_text, re.DOTALL | re.MULTILINE)
            for match in reversed(list(matches)):
                original = match.group(0)
                placeholder = self._generate_placeholder(prefix)
                self.placeholder_map[placeholder] = original
                
                protected_text = (
                    protected_text[:match.start()] + 
                    placeholder + 
                    protected_text[match.end():]
                )
        
        return protected_text
    
    def restore_latex_commands(self, text: str) -> str:
        """è¿˜åŸ LaTeX å‘½ä»¤"""
        restored_text = text
        for placeholder, original in self.placeholder_map.items():
            restored_text = restored_text.replace(placeholder, original)
        return restored_text
    
    def split_into_chunks(
        self, 
        text: str, 
        max_length: int = 2000,  # ğŸ”§ å¢åŠ åˆ° 2000 å­—ç¬¦
        min_length: int = 500    # ğŸ”§ å¢åŠ æœ€å°é•¿åº¦
    ) -> List[str]:
        """
        æ™ºèƒ½åˆ‡åˆ†æ–‡æœ¬ï¼ˆç®€åŒ–ç‰ˆ - è®© LLM å¤„ç†ç»“æ„ï¼‰
        
        ç­–ç•¥ï¼š
        1. æŒ‰æ®µè½è‡ªç„¶åˆ‡åˆ†ï¼ˆ\n\nï¼‰
        2. ä¸å†ç‰¹æ®Šä¿æŠ¤è¡¨æ ¼ã€åˆ—è¡¨ç­‰ç¯å¢ƒ
        3. è®© LLM è‡ªè¡Œç†è§£å’Œç¿»è¯‘è¿™äº›ç»“æ„
        """
        chunks = []
        
        # æŒ‰æ®µè½åˆ‡åˆ†
        paragraphs = re.split(r'\n\s*\n', text)
        
        current_chunk = ""
        
        for para in paragraphs:
            para = para.strip()
            if not para:
                continue
            
            # å¦‚æœåŠ å…¥å½“å‰æ®µè½ä¸è¶…è¿‡é™åˆ¶ï¼Œå°±æ·»åŠ 
            if len(current_chunk) + len(para) + 2 < max_length:
                current_chunk += ("\n\n" if current_chunk else "") + para
            else:
                # ä¿å­˜å½“å‰å—
                if current_chunk:
                    chunks.append(current_chunk)
                
                # å¦‚æœå•ä¸ªæ®µè½è¶…é•¿ï¼Œéœ€è¦è¿›ä¸€æ­¥åˆ‡åˆ†
                if len(para) > max_length:
                    # æŒ‰å¥å­åˆ‡åˆ†ï¼ˆç®€å•å¤„ç†ï¼‰
                    sentences = re.split(r'(?<=[.!?])\s+', para)
                    temp_chunk = ""
                    for sent in sentences:
                        if len(temp_chunk) + len(sent) < max_length:
                            temp_chunk += (" " if temp_chunk else "") + sent
                        else:
                            if temp_chunk:
                                chunks.append(temp_chunk)
                            temp_chunk = sent
                    if temp_chunk:
                        chunks.append(temp_chunk)
                else:
                    current_chunk = para
        
        # æ·»åŠ æœ€åä¸€å—
        if current_chunk:
            chunks.append(current_chunk)
        
        return chunks if chunks else [""]


def build_latex_translation_prompt(source_lang: str, target_lang: str) -> str:
    """æ„å»ºé€‚é… LLM çš„ LaTeX ç¿»è¯‘æç¤ºè¯"""
    
    if target_lang == 'English':
        lang_guide = "Use formal academic English with standard terminology"
    else:
        lang_guide = "ä½¿ç”¨è§„èŒƒå­¦æœ¯ä¸­æ–‡ï¼Œæœ¯è¯­å‡†ç¡®"
    
    prompt = f"""You are a professional LaTeX document translator. Translate {source_lang} to {target_lang}.

**Critical Rules**:
1. **Preserve ALL placeholders** exactly as-is: <CITE_1>, <REF_2>, <LABEL_3>, <DOCUMENTCLASS_1>, etc.
2. **Preserve LaTeX delimiters**: :::, $$, \\begin{{...}}, \\end{{...}}
3. **Translate natural language** while keeping:
   - LaTeX commands unchanged
   - Math expressions unchanged
   - Code blocks unchanged
   - All placeholders unchanged

4. **Handle structures naturally**:
   - Tables (tabular/table): Translate content, keep LaTeX structure
   - Lists (itemize/enumerate): Translate items, keep structure
   - Sections/subsections: Translate titles, keep commands
   - Figures: Translate captions, keep structure

5. **Output requirements**:
   - {lang_guide}
   - Keep original formatting (line breaks, indentation)
   - Translate ALL text content (don't skip paragraphs)
   - Output ONLY the translation, no explanations

**Example**:
Input: "è¿™æ˜¯ä¸€ä¸ª<REF_1>ç¤ºä¾‹\\cite{{example}}ã€‚"
Output: "This is a <REF_1> example\\cite{{example}}."

Now translate the following text:"""
    
    return prompt


def call_llm_api(
    text: str,
    system_prompt: str,
    model: str,
    api_base: str,
    api_key: str,
    timeout: int = 180,
    max_retries: int = 3,
    interval: float = 0.5
) -> str:
    """è°ƒç”¨ LLM API è¿›è¡Œç¿»è¯‘"""
    url = f"{api_base.rstrip('/')}/chat/completions"
    
    headers = {
        "Content-Type": "application/json",
        "Authorization": f"Bearer {api_key}"
    }
    
    payload = {
        "model": model,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": text}
        ],
        "temperature": 0.3,  # ğŸ”§ ç•¥å¾®æé«˜æ¸©åº¦ï¼Œè®©ç¿»è¯‘æ›´è‡ªç„¶
        "top_p": 0.9,
        "max_tokens": int(len(text) * 2.0),  # ğŸ”§ å¢åŠ è¾“å‡ºé•¿åº¦é™åˆ¶
    }
    
    last_error = None
    
    for attempt in range(max_retries):
        try:
            response = requests.post(
                url,
                headers=headers,
                json=payload,
                timeout=timeout
            )
            
            if response.status_code != 200:
                last_error = f"HTTP {response.status_code}: {response.text}"
                logger.warning(f"   API error, retrying... ({attempt + 1}/{max_retries})")
                
                if attempt < max_retries - 1:
                    time.sleep(interval * (attempt + 1))
                continue
            
            result = response.json()
            
            if "error" in result:
                last_error = result["error"].get("message", str(result["error"]))
                logger.warning(f"   API error: {last_error}, retrying...")
                
                if attempt < max_retries - 1:
                    time.sleep(interval * (attempt + 1))
                continue
            
            if "choices" in result and len(result["choices"]) > 0:
                translated_text = result["choices"][0]["message"]["content"].strip()
                
                finish_reason = result["choices"][0].get("finish_reason", "")
                if finish_reason == "length":
                    logger.warning("   âš ï¸ Output was truncated due to length limit!")
                
                usage = result.get("usage", {})
                if usage:
                    logger.debug(f"   Token usage: {usage.get('prompt_tokens', 0)} prompt + "
                               f"{usage.get('completion_tokens', 0)} completion = "
                               f"{usage.get('total_tokens', 0)} total")
                
                return translated_text
            else:
                last_error = "No choices in API response"
                
                if attempt < max_retries - 1:
                    time.sleep(interval)
                continue
            
        except requests.exceptions.Timeout:
            last_error = "Request timeout"
            logger.warning(f"   Timeout, retrying... ({attempt + 1}/{max_retries})")
            
            if attempt < max_retries - 1:
                time.sleep(interval * 2)
            continue
        
        except Exception as e:
            last_error = str(e)
            logger.warning(f"   Error: {e}, retrying...")
            
            if attempt < max_retries - 1:
                time.sleep(interval)
            continue
    
    raise Exception(f"Translation failed after {max_retries} attempts. Last error: {last_error}")


def find_referenced_files(tex_file: str, base_dir: str, visited: set = None) -> List[str]:
    """é€’å½’æŸ¥æ‰¾å¼•ç”¨çš„ .tex æ–‡ä»¶"""
    if visited is None:
        visited = set()
    
    tex_file_abs = os.path.abspath(tex_file)
    if tex_file_abs in visited:
        return []
    
    visited.add(tex_file_abs)
    referenced_files = []
    
    try:
        with open(tex_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        for pattern in [r'\\input\{([^}]+)\}', r'\\include\{([^}]+)\}']:
            for match in re.findall(pattern, content):
                ref_file = match.strip()
                if not ref_file.endswith('.tex'):
                    ref_file += '.tex'
                
                ref_path = ref_file if os.path.isabs(ref_file) else os.path.join(base_dir, ref_file)
                
                if os.path.exists(ref_path):
                    referenced_files.append(ref_path)
                    sub_refs = find_referenced_files(ref_path, os.path.dirname(ref_path), visited)
                    referenced_files.extend(sub_refs)
                else:
                    logger.warning(f"Referenced file not found: {ref_path}")
    
    except Exception as e:
        logger.error(f"Error reading {tex_file}: {e}")
    
    return referenced_files
def convert_article_to_ctexart(content: str, direction: str) -> str:
    """è‹±è¯‘ä¸­æ—¶è½¬æ¢ä¸º ctexart å¹¶é…ç½®å­—ä½“"""
    if direction != 'en-to-zh':
        return content
    
    # 1. è½¬æ¢æ–‡æ¡£ç±»
    content = re.sub(
        r'\\documentclass(\[.*?\])?\{article\}',
        r'\\documentclass\1{ctexart}',
        content
    )
    
    # 2. æ·»åŠ å­—ä½“æ”¯æŒï¼ˆå¦‚æœæ²¡æœ‰ï¼‰
    if 'ctexart' in content and 'xeCJK' not in content:
        font_settings = r"""
% ==================== ä¸­æ–‡å­—ä½“é…ç½® ====================
\usepackage{xeCJK}
\usepackage{fontspec}

% Windows ç³»ç»Ÿå­—ä½“
\setCJKmainfont{SimSun}[
    BoldFont=SimHei,
    ItalicFont=KaiTi
]
\setCJKsansfont{SimHei}
\setCJKmonofont{FangSong}

% è‹±æ–‡å­—ä½“
\setmainfont{Times New Roman}
\setsansfont{Arial}
\setmonofont{Courier New}

% æ•°å­¦å­—ä½“ï¼ˆé¿å…ä¸­æ–‡æ±¡æŸ“æ•°å­¦ç¯å¢ƒï¼‰
\usepackage{amsmath}
\usepackage{amssymb}
% ====================================================
"""
        # åœ¨ \begin{document} å‰æ’å…¥
        content = content.replace(r'\begin{document}', font_settings + r'\begin{document}')
    
    return content
def translate_latex_file(
    input_file: str,
    output_file: str,
    model: str,
    api_base: str,
    api_key: str,
    source_lang: str,
    target_lang: str,
    direction: str,
    cache: TranslationCache,
    timeout: int = 180,
    max_retries: int = 3,
    interval: float = 0.5
) -> bool:
    """ç¿»è¯‘å•ä¸ª LaTeX æ–‡ä»¶ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†ï¼‰"""
    try:
        logger.info(f"ğŸ“„ Translating: {os.path.basename(input_file)}")
        
        try:
            with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
                content = f.read()
        except Exception as e:
            logger.error(f"   âŒ Failed to read file: {e}")
            return False
        
        original_length = len(content)
        logger.info(f"   Original length: {original_length} characters")
        
        translator = ClsStyTranslator()
        
        try:
            protected_content = translator.protect_latex_commands(content)
            logger.info(f"   Protected {len(translator.placeholder_map)} LaTeX elements")
        except Exception as e:
            logger.error(f"   âŒ Protection failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
        
        try:
            chunks = translator.split_into_chunks(protected_content, max_length=1500, min_length=200)
            logger.info(f"   Split into {len(chunks)} chunks")
        except Exception as e:
            logger.error(f"   âŒ Splitting failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
        
        for i, chunk in enumerate(chunks):
            logger.debug(f"   Chunk {i+1}: {len(chunk)} chars")
        
        system_prompt = build_latex_translation_prompt(source_lang, target_lang)
        
        translated_chunks = []
        cache_hits = 0
        failed_chunks = []
        
        for i, chunk in enumerate(chunks):
            if not chunk.strip():
                translated_chunks.append(chunk)
                continue
            
            try:
                logger.info(f"   ğŸ”„ Processing chunk {i+1}/{len(chunks)}...")
                
                if len(chunk) > 10000:
                    logger.warning(f"   âš ï¸ Chunk {i+1} is very large ({len(chunk)} chars), may fail")
                
                try:
                    cached_translation = cache.get(chunk, model, direction)
                except Exception as e:
                    logger.warning(f"   âš ï¸ Cache read failed: {e}, skipping cache")
                    cached_translation = None
                
                if cached_translation:
                    translated_chunks.append(cached_translation)
                    cache_hits += 1
                    logger.info(f"   â™»ï¸ Chunk {i+1}/{len(chunks)} (cached)")
                else:
                    try:
                        translated = call_llm_api(
                            text=chunk,
                            system_prompt=system_prompt,
                            model=model,
                            api_base=api_base,
                            api_key=api_key,
                            timeout=timeout,
                            max_retries=max_retries,
                            interval=interval
                        )
                        
                        if not translated or len(translated) < 10:
                            logger.warning(f"   âš ï¸ Chunk {i+1} returned suspiciously short translation!")
                            logger.warning(f"   Original length: {len(chunk)}, translated: {len(translated) if translated else 0}")
                            failed_chunks.append(i+1)
                            translated_chunks.append(chunk)
                        else:
                            try:
                                cache.set(chunk, model, direction, translated)
                            except Exception as e:
                                logger.warning(f"   âš ï¸ Cache write failed: {e}")
                            
                            translated_chunks.append(translated)
                            logger.info(f"   âœ“ Chunk {i+1}/{len(chunks)} (translated, {len(translated)} chars)")
                    
                    except Exception as e:
                        logger.error(f"   âœ— Chunk {i+1} API call failed: {e}")
                        import traceback
                        logger.error(traceback.format_exc())
                        translated_chunks.append(chunk)
                        failed_chunks.append(i+1)
                
            except Exception as e:
                logger.error(f"   âœ— Chunk {i+1} processing failed: {e}")
                import traceback
                logger.error(traceback.format_exc())
                translated_chunks.append(chunk)
                failed_chunks.append(i+1)
        
        if cache_hits > 0:
            logger.info(f"   ğŸ“Š Cache hits: {cache_hits}/{len(chunks)} "
                       f"({cache_hits/len(chunks)*100:.1f}% saved)")
        
        if failed_chunks:
            logger.warning(f"   âš ï¸ Failed/suspicious chunks: {failed_chunks}")
        
        try:
            merged = "\n\n".join(translated_chunks)
            final_content = translator.restore_latex_commands(merged)
        except Exception as e:
            logger.error(f"   âŒ Merging/restoration failed: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return False
        
        translated_length = len(final_content)
        logger.info(f"   Translated length: {translated_length} characters "
                   f"({translated_length/original_length*100:.1f}% of original)")
        
        try:
            final_content = convert_article_to_ctexart(final_content, direction)
        except Exception as e:
           logger.warning(f"   âš ï¸ Format conversion failed: {e}, using original")
       
        try:
           os.makedirs(os.path.dirname(output_file) or '.', exist_ok=True)
           with open(output_file, 'w', encoding='utf-8') as f:
               f.write(final_content)
           logger.info(f"   âœ… Saved: {os.path.basename(output_file)}")
        except Exception as e:
           logger.error(f"   âŒ Save failed: {e}")
           import traceback
           logger.error(traceback.format_exc())
           return False
       
        return True
       
    except Exception as e:
       logger.error(f"   âŒ File translation failed: {e}")
       import traceback
       logger.error(traceback.format_exc())
       return False


def test_api_connection(api_base: str, api_key: str) -> bool:
   """æµ‹è¯• API è¿æ¥"""
   try:
       url = f"{api_base.rstrip('/')}/models"
       headers = {"Authorization": f"Bearer {api_key}"}
       
       response = requests.get(url, headers=headers, timeout=5)
       
       if response.status_code == 200:
           logger.info("âœ… API connection test passed")
           return True
       else:
           logger.error(f"âŒ API connection failed: HTTP {response.status_code}")
           return False
   except Exception as e:
       logger.error(f"âŒ API connection test failed: {e}")
       return False
def copy_all_project_files(source_dir: str, dest_dir: str, processed_files: List[str] = None):
    """
    å¤åˆ¶é¡¹ç›®æ‰€æœ‰æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•ï¼ˆä¿®å¤ç‰ˆ - æ’é™¤å·²å¤„ç†çš„æ–‡ä»¶ï¼‰
    
    Args:
        source_dir: æºé¡¹ç›®æ ¹ç›®å½•
        dest_dir: ç›®æ ‡ç›®å½•
        processed_files: å·²ç¿»è¯‘çš„æ–‡ä»¶åˆ—è¡¨ï¼ˆç»å¯¹è·¯å¾„ï¼‰ï¼Œè¿™äº›æ–‡ä»¶ä¸ä¼šè¢«è¦†ç›–
    """
    # æ’é™¤çš„ç›®å½•
    exclude_dirs = {
        '.git', '.svn', '__pycache__', 'node_modules',
        '.vscode', '.idea', 'build', 'dist', '__MACOSX'
    }
    
    # æ’é™¤çš„ä¸´æ—¶æ–‡ä»¶æ‰©å±•å
    exclude_extensions = {
        '.aux', '.log', '.out', '.toc', '.synctex.gz',
        '.fdb_latexmk', '.fls', '.bbl', '.blg', '.bcf',
        '.run.xml', '.nav', '.snm', '.vrb', '.lof', '.lot',
        '.bak', '.swp', '.tmp', '~', '.xdv'
    }
    
    copied_files = 0
    copied_dirs = 0
    skipped_files = 0
    skipped_processed = 0
    
    logger.info("\nğŸ“¦ Copying remaining project files...")
    
    # è½¬æ¢ processed_files ä¸ºç»å¯¹è·¯å¾„é›†åˆ
    processed_set = set()
    if processed_files:
        processed_set = {os.path.abspath(f) for f in processed_files}
    
    # è·å–æºç›®å½•å’Œç›®æ ‡ç›®å½•çš„ç»å¯¹è·¯å¾„
    source_dir_abs = os.path.abspath(source_dir)
    dest_dir_abs = os.path.abspath(dest_dir)
    
    for root, dirs, files in os.walk(source_dir):
        # è¿‡æ»¤æ’é™¤çš„ç›®å½•
        dirs[:] = [d for d in dirs if d not in exclude_dirs and not d.startswith('.')]
        
        # è·³è¿‡è¾“å‡ºç›®å½•æœ¬èº«
        root_abs = os.path.abspath(root)
        if root_abs == dest_dir_abs or root_abs.startswith(dest_dir_abs + os.sep):
            continue
        
        # è®¡ç®—ç›¸å¯¹è·¯å¾„
        rel_dir = os.path.relpath(root, source_dir)
        dest_subdir = os.path.join(dest_dir, rel_dir) if rel_dir != '.' else dest_dir
        
        # åˆ›å»ºç›®æ ‡å­ç›®å½•
        try:
            if not os.path.exists(dest_subdir):
                os.makedirs(dest_subdir, exist_ok=True)
                copied_dirs += 1
                logger.debug(f"   ğŸ“ Created directory: {rel_dir}")
        except Exception as e:
            logger.warning(f"   âš ï¸ Failed to create directory {rel_dir}: {e}")
            continue
        
        # å¤åˆ¶æ–‡ä»¶
        for file in files:
            source_file = os.path.join(root, file)
            source_file_abs = os.path.abspath(source_file)
            rel_path = os.path.relpath(source_file, source_dir)
            dest_file = os.path.join(dest_subdir, file)
            
            # è·å–æ–‡ä»¶æ‰©å±•å
            _, ext = os.path.splitext(file)
            ext_lower = ext.lower()
            
            # è·³è¿‡éšè—æ–‡ä»¶å’Œä¸´æ—¶æ–‡ä»¶
            if file.startswith('.') or ext_lower in exclude_extensions:
                skipped_files += 1
                continue
            
            # ğŸ†• å…³é”®ä¿®å¤ï¼šè·³è¿‡å·²ç»ç¿»è¯‘è¿‡çš„æ–‡ä»¶
            if source_file_abs in processed_set:
                skipped_processed += 1
                logger.debug(f"   â­ï¸ Skipped processed file: {rel_path}")
                continue
            
            # ğŸ†• æ£€æŸ¥ç›®æ ‡æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨ï¼ˆè¢«ç¿»è¯‘ç”Ÿæˆï¼‰
            if os.path.exists(dest_file):
                # å¦‚æœç›®æ ‡æ–‡ä»¶å­˜åœ¨ä¸”æ¯”æºæ–‡ä»¶æ–°ï¼Œè¯´æ˜æ˜¯ç¿»è¯‘ç”Ÿæˆçš„ï¼Œä¸è¦†ç›–
                if os.path.getmtime(dest_file) > os.path.getmtime(source_file):
                    skipped_processed += 1
                    logger.debug(f"   â­ï¸ Skipped existing translated file: {rel_path}")
                    continue
            
            # å¤åˆ¶å…¶ä»–æ‰€æœ‰æ–‡ä»¶
            try:
                shutil.copy2(source_file, dest_file)
                copied_files += 1
                
                # è®°å½•é‡è¦æ–‡ä»¶ç±»å‹
                important_extensions = {
                    '.bib', '.cls', '.sty', '.bst',      # æ ·å¼æ–‡ä»¶
                    '.jpg', '.jpeg', '.png', '.pdf',     # å›¾ç‰‡
                    '.eps', '.svg', '.tif', '.tiff',     # æ›´å¤šå›¾ç‰‡æ ¼å¼
                    '.bat', '.sh',                       # è„šæœ¬æ–‡ä»¶
                }
                if ext_lower in important_extensions:
                    logger.debug(f"   âœ“ Copied: {rel_path}")
                
            except Exception as e:
                logger.warning(f"   âš ï¸ Failed to copy {rel_path}: {e}")
    
    # æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
    logger.info(f"   âœ… Copied {copied_files} files")
    logger.info(f"   ğŸ“ Created {copied_dirs} directories")
    logger.info(f"   â­ï¸ Skipped {skipped_processed} processed files (already translated)")
    logger.info(f"   ğŸ—‘ï¸ Skipped {skipped_files} temporary/hidden files")
    
    # æ˜¾ç¤ºæ–‡ä»¶ç±»å‹ç»Ÿè®¡
    show_file_type_stats(dest_dir)

def show_file_type_stats(directory: str):
    """æ˜¾ç¤ºç›®å½•ä¸­çš„æ–‡ä»¶ç±»å‹ç»Ÿè®¡"""
    file_types = {}
    dir_count = 0
    
    for root, dirs, files in os.walk(directory):
        dir_count += len(dirs)
        for file in files:
            _, ext = os.path.splitext(file)
            ext = ext.lower() if ext else '(no extension)'
            file_types[ext] = file_types.get(ext, 0) + 1
    
    if file_types or dir_count > 0:
        logger.info("\n   ğŸ“Š Output directory statistics:")
        logger.info(f"      Total directories: {dir_count}")
        logger.info(f"      Total files: {sum(file_types.values())}")
        
        # æŒ‰æ•°é‡æ’åºï¼Œæ˜¾ç¤ºå‰ 15 ç§
        sorted_types = sorted(file_types.items(), key=lambda x: x[1], reverse=True)
        logger.info("\n   File types (top 15):")
        for ext, count in sorted_types[:15]:
            logger.info(f"      {ext:20s}: {count:3d} files")
def translate_style_file(
    input_file: str,
    output_file: str,
    model: str,
    api_base: str,
    api_key: str,
    source_lang: str,
    target_lang: str,
    direction: str,
    cache: TranslationCache,
    timeout: int = 180,
    max_retries: int = 3,
    interval: float = 0.5
) -> bool:
    """
    ç¿»è¯‘æ ·å¼æ–‡ä»¶ï¼ˆ.cls/.styï¼‰
    
    ç­–ç•¥ï¼š
    1. åªç¿»è¯‘æ³¨é‡Šå’Œä¸­æ–‡å­—ç¬¦ä¸²
    2. å®Œå…¨ä¿ç•™ LaTeX å‘½ä»¤ç»“æ„
    3. é€è¡Œæ›¿æ¢ç¿»è¯‘ç»“æœ
    """
    try:
        logger.info(f"ğŸ“„ Translating style file: {os.path.basename(input_file)}")
        
        # è¯»å–åŸæ–‡ä»¶
        with open(input_file, 'r', encoding='utf-8', errors='ignore') as f:
            original_content = f.read()
        
        translator = StyleFileTranslator()
        translatable_parts = translator.extract_translatable_parts(original_content)
        
        if not translatable_parts:
            logger.info(f"   â„¹ï¸ No translatable content found, copying file as-is")
            os.makedirs(os.path.dirname(output_file), exist_ok=True)
            shutil.copy2(input_file, output_file)
            return True
        
        logger.info(f"   Found {len(translatable_parts)} translatable segments")
        
        # æ„å»ºç¿»è¯‘æ˜ å°„
        translations = {}
        system_prompt = build_style_file_translation_prompt(source_lang, target_lang)
        
        # åˆ†å—ç¿»è¯‘
        chunks = translator.split_into_chunks(original_content)
        logger.info(f"   Split into {len(chunks)} chunks")
        
        for i, chunk in enumerate(chunks):
            if not chunk.strip():
                continue
            
            logger.info(f"   ğŸ”„ Processing chunk {i+1}/{len(chunks)}...")
            
            # æ£€æŸ¥ç¼“å­˜
            cached = cache.get(chunk, model, direction)
            if cached:
                translated = cached
                logger.info(f"   â™»ï¸ Chunk {i+1} (cached)")
            else:
                try:
                    translated = call_llm_api(
                        text=chunk,
                        system_prompt=system_prompt,
                        model=model,
                        api_base=api_base,
                        api_key=api_key,
                        timeout=timeout,
                        max_retries=max_retries,
                        interval=interval
                    )
                    cache.set(chunk, model, direction, translated)
                    logger.info(f"   âœ“ Chunk {i+1} (translated)")
                except Exception as e:
                    logger.error(f"   âœ— Chunk {i+1} failed: {e}")
                    continue
            
            # è§£æç¿»è¯‘ç»“æœï¼Œæ„å»ºæ˜ å°„
            # æ ¼å¼ï¼š[comment] åŸæ–‡ -> è¯‘æ–‡
            for line in translated.split('\n'):
                match = re.match(r'\[(comment|chinese_string)\]\s*(.+)', line)
                if match:
                    typ, translated_text = match.groups()
                    # åœ¨ translatable_parts ä¸­æ‰¾åˆ°å¯¹åº”çš„åŸæ–‡
                    for start, end, original_text, part_type in translatable_parts:
                        if part_type == typ and original_text.strip() == translated_text.strip():
                            translations[original_text] = translated_text
                            break
        
        # æ›¿æ¢åŸæ–‡ä¸­çš„ç¿»è¯‘éƒ¨åˆ†
        result_content = original_content
        for original, translated in translations.items():
            result_content = result_content.replace(original, translated)
        
        # ä¿å­˜ç»“æœ
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, 'w', encoding='utf-8') as f:
            f.write(result_content)
        
        logger.info(f"   âœ… Saved: {os.path.basename(output_file)}")
        return True
    
    except Exception as e:
        logger.error(f"   âŒ Style file translation failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def translate_cls_or_sty_file_wrapper(
    input_file: str,
    output_file: str,
    api_base: str,
    api_key: str,
    model: str,
    direction: str,
    verbose: bool = True
) -> bool:
    """
    ä½¿ç”¨ latex_translation.py ç¿»è¯‘ .cls æ–‡ä»¶
    
    :param input_file: è¾“å…¥ .cls æ–‡ä»¶
    :param output_file: è¾“å‡º .cls æ–‡ä»¶
    :param api_base: API åœ°å€
    :param api_key: API å¯†é’¥
    :param model: æ¨¡å‹åç§°
    :param direction: ç¿»è¯‘æ–¹å‘ï¼ˆzh-to-en æˆ– en-to-zhï¼‰
    :param verbose: æ˜¯å¦æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    :return: æ˜¯å¦æˆåŠŸ
    """
    if not CLS_TRANSLATOR_AVAILABLE:
        logger.error("âŒ latex_translation module not available, cannot translate .cls files")
        return False
    
    try:
        logger.info(f"ğŸ“„ Translating .cls file: {os.path.basename(input_file)}")
        
        # è°ƒç”¨ä½ çš„ç¿»è¯‘å™¨
        result = translate_cls_or_sty_file(
            input_file=input_file,
            output_file=output_file,
            api_key=api_key,
            model=model,
            base_url=api_base,
            max_tokens_per_group=2000,
            verbose=verbose
        )
        
        if result['success']:
            logger.info(f"   âœ… Translated {result['blocks_translated']} blocks")
            logger.info(f"   ğŸ’¾ Saved: {os.path.basename(output_file)}")
            return True
        else:
            logger.error(f"   âŒ Translation failed")
            return False
            
    except Exception as e:
        logger.error(f"   âŒ CLS translation error: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False

def build_style_file_translation_prompt(source_lang: str, target_lang: str) -> str:
    """æ ·å¼æ–‡ä»¶ä¸“ç”¨ç¿»è¯‘æç¤ºè¯"""
    
    prompt = f"""You are translating comments and Chinese strings from a LaTeX style file (.cls/.sty).

**Critical Rules**:
1. Input format: Each line is `[type] text`, where type is 'comment' or 'chinese_string'
2. **Only translate the text part**, keep the `[type]` prefix
3. Output format: Same as input, one line per entry
4. Keep LaTeX commands (\\xxx) unchanged
5. Keep special characters (~, {{}}, []) unchanged
6. Translate from {source_lang} to {target_lang}

**Example**:
Input:
[comment] è¡¨æ ¼ååŠå›¾å
[chinese_string] å®šä¹‰~
[comment] å»æ‰å›¾æ ‡ç­¾åçš„å†’å·

Output:
[comment] Table name and figure name
[chinese_string] Definition~
[comment] Remove colon after figure label

Now translate:"""
    
    return prompt
def translate_latex_project(
    input_main_file: str,
    output_dir: str,
    translate_function,
    translate_kwargs: dict,
    translate_style_files: bool = False,
    progress_callback=None
) -> bool:
    """ç¿»è¯‘æ•´ä¸ª LaTeX é¡¹ç›®ï¼ˆæ”¯æŒ CLS ç¿»è¯‘ï¼‰"""
    
    if not test_api_connection(translate_kwargs['api_base'], translate_kwargs['api_key']):
        logger.error("API connection test failed, aborting")
        return False
    
    cache = TranslationCache(
        max_age_days=translate_kwargs.get('cache_max_age_days', 30),
        max_entries=translate_kwargs.get('cache_max_entries', 10000)
    )
    
    try:
        direction = "zh-to-en" if translate_kwargs['source_lang'] == 'Chinese' else "en-to-zh"
        
        logger.info("=" * 80)
        logger.info(f"ğŸš€ LaTeX Project Translation: {direction}")
        logger.info(f"ğŸ“‚ Main file: {os.path.basename(input_main_file)}")
        logger.info(f"âš™ï¸  Style files translation: {'Enabled' if translate_style_files else 'Disabled'}")
        if translate_style_files and CLS_TRANSLATOR_AVAILABLE:
            logger.info(f"ğŸ”§ Using enhanced CLS translator (latex_translation.py)")
        logger.info("=" * 80)
        
        project_root = os.path.dirname(os.path.abspath(input_main_file))
        
        # 1. æŸ¥æ‰¾æ‰€æœ‰ .tex æ–‡ä»¶
        all_tex_files = [input_main_file]
        referenced_files = find_referenced_files(input_main_file, project_root)
        all_tex_files.extend(referenced_files)
        all_tex_files = list(dict.fromkeys(all_tex_files))
        
        logger.info(f"\nğŸ“š Found {len(all_tex_files)} .tex files")
        
        # 2. ğŸ†• æŸ¥æ‰¾æ ·å¼æ–‡ä»¶ï¼ˆåŒºåˆ† CLS å’Œ STYï¼‰
        style_files_dict = {'cls': [], 'sty': []}
        if translate_style_files:
            logger.info("\nğŸ” Searching for style files...")
            style_files_dict = find_style_files(input_main_file, project_root)
            
            total_style_files = len(style_files_dict['cls']) + len(style_files_dict['sty'])
            
            if total_style_files > 0:
                logger.info(f"ğŸ“‹ Found {total_style_files} style files:")
                if style_files_dict['cls']:
                    logger.info(f"   â€¢ {len(style_files_dict['cls'])} .cls files")
                    for cls in style_files_dict['cls']:
                        logger.info(f"     - {os.path.relpath(cls, project_root)}")
                if style_files_dict['sty']:
                    logger.info(f"   â€¢ {len(style_files_dict['sty'])} .sty files")
                    for sty in style_files_dict['sty']:
                        logger.info(f"     - {os.path.relpath(sty, project_root)}")
            else:
                logger.info("   â„¹ï¸ No local style files found")
        
        
        os.makedirs(output_dir, exist_ok=True)
        all_processed_files = []
        
        # 3. ç¿»è¯‘ .tex æ–‡ä»¶
        success_count = 0
        total_style_files = len(style_files_dict['cls']) + len(style_files_dict['sty'])
        total_files = len(all_tex_files) + total_style_files
        
        for idx, tex_file in enumerate(all_tex_files, 1):
            logger.info(f"\n{'='*80}")
            logger.info(f"[{idx}/{total_files}] Processing .tex file...")
            
            rel_path = os.path.relpath(tex_file, project_root)
            output_file = os.path.join(output_dir, rel_path)
            
            if progress_callback:
                progress_callback(
                    current_file=os.path.basename(tex_file),
                    current=idx,
                    total=total_files,
                    message=f"Translating {rel_path}"
                )
            
            if translate_latex_file(
                input_file=tex_file,
                output_file=output_file,
                model=translate_kwargs['model'],
                api_base=translate_kwargs['api_base'],
                api_key=translate_kwargs['api_key'],
                source_lang=translate_kwargs['source_lang'],
                target_lang=translate_kwargs['target_lang'],
                direction=direction,
                cache=cache,
                timeout=translate_kwargs.get('timeout', 180),
                max_retries=translate_kwargs.get('max_retries', 3),
                interval=translate_kwargs.get('interval', 0.5)
            ):
                success_count += 1
                all_processed_files.append(tex_file)  # ğŸ†• è®°å½•å·²å¤„ç†æ–‡ä»¶
        
        # 4. ç¿»è¯‘ .cls æ–‡ä»¶
        if translate_style_files and style_files_dict['cls']:
            logger.info(f"\n{'='*80}")
            logger.info("ğŸ“‹ Translating .cls files with enhanced translator...")
            
            for idx, cls_file in enumerate(style_files_dict['cls'], len(all_tex_files) + 1):
                logger.info(f"\n{'='*80}")
                logger.info(f"[{idx}/{total_files}] Processing .cls file...")
                
                rel_path = os.path.relpath(cls_file, project_root)
                output_file = os.path.join(output_dir, rel_path)
                
                if progress_callback:
                    progress_callback(
                        current_file=os.path.basename(cls_file),
                        current=idx,
                        total=total_files,
                        message=f"Translating {rel_path}"
                    )
                
                if translate_cls_or_sty_file_wrapper(
                    input_file=cls_file,
                    output_file=output_file,
                    api_base=translate_kwargs['api_base'],
                    api_key=translate_kwargs['api_key'],
                    model=translate_kwargs['model'],
                    direction=direction,
                    verbose=True
                ):
                    success_count += 1
                    all_processed_files.append(cls_file)  # ğŸ†• è®°å½•å·²å¤„ç†æ–‡ä»¶
        
        # 5. ç¿»è¯‘ .sty æ–‡ä»¶ï¼ˆå¦‚æœæœ‰ï¼‰
        if translate_style_files and style_files_dict['sty']:
            logger.info(f"\n{'='*80}")
            logger.info("ğŸ“‹ Translating .sty files with enhanced translator...")
            
            current_idx = len(all_tex_files) + len(style_files_dict['cls']) + 1
            
            for idx, sty_file in enumerate(style_files_dict['sty'], current_idx):
                logger.info(f"\n{'='*80}")
                logger.info(f"[{idx}/{total_files}] Processing .sty file...")
                
                rel_path = os.path.relpath(sty_file, project_root)
                output_file = os.path.join(output_dir, rel_path)
                
                if progress_callback:
                    progress_callback(
                        current_file=os.path.basename(sty_file),
                        current=idx,
                        total=total_files,
                        message=f"Translating {rel_path}"
                    )
                
                # ğŸ†• ä½¿ç”¨ä¸“ç”¨ç¿»è¯‘å™¨ï¼ˆä¸ .cls ç›¸åŒçš„å¤„ç†æ–¹å¼ï¼‰
                if translate_cls_or_sty_file_wrapper(
                    input_file=sty_file,
                    output_file=output_file,
                    api_base=translate_kwargs['api_base'],
                    api_key=translate_kwargs['api_key'],
                    model=translate_kwargs['model'],
                    direction=direction,
                    verbose=True
                ):
                    success_count += 1
                    all_processed_files.append(sty_file)  # è®°å½•å·²å¤„ç†æ–‡ä»¶
                else:
                    logger.error(f"   âŒ Failed to translate .sty file")
        
        # 6. å¤åˆ¶å…¶ä»–æ–‡ä»¶
        logger.info("\n" + "=" * 80)
        if not translate_style_files:
            # å¦‚æœä¸ç¿»è¯‘æ ·å¼æ–‡ä»¶ï¼Œéœ€è¦å¤åˆ¶å®ƒä»¬
            logger.info("ğŸ“‹ Copying style files without translation...")
            for cls_file in style_files_dict['cls']:
                rel_path = os.path.relpath(cls_file, project_root)
                dest_file = os.path.join(output_dir, rel_path)
                try:
                    os.makedirs(os.path.dirname(dest_file), exist_ok=True)
                    shutil.copy2(cls_file, dest_file)
                    logger.info(f"   âœ“ Copied .cls: {rel_path}")
                except Exception as e:
                    logger.warning(f"   âš ï¸ Failed to copy {rel_path}: {e}")
            
            for sty_file in style_files_dict['sty']:
                rel_path = os.path.relpath(sty_file, project_root)
                dest_file = os.path.join(output_dir, rel_path)
                try:
                    os.makedirs(os.path.dirname(dest_file), exist_ok=True)
                    shutil.copy2(sty_file, dest_file)
                    logger.info(f"   âœ“ Copied .sty: {rel_path}")
                except Exception as e:
                    logger.warning(f"   âš ï¸ Failed to copy {rel_path}: {e}")
        
        # ğŸ†• å…³é”®ä¿®å¤ï¼šä¼ é€’å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨ï¼Œé¿å…è¦†ç›–
        copy_all_project_files(
            project_root, 
            output_dir, 
            processed_files=all_processed_files  # ä¼ é€’å·²å¤„ç†æ–‡ä»¶åˆ—è¡¨
        )
        
        logger.info("\n" + "=" * 80)
        logger.info(f"âœ… Translation Complete: {success_count}/{total_files} files succeeded")
        logger.info(f"ğŸ“ Output directory: {os.path.abspath(output_dir)}")
        logger.info("=" * 80)
        
        return success_count == total_files
    
    except Exception as e:
        logger.error(f"âŒ Project translation failed: {e}")
        import traceback
        logger.error(traceback.format_exc())
        return False
    
    finally:
        cache.close()
class StyleFileTranslator(ClsStyTranslator):
    """
    æ ·å¼æ–‡ä»¶ç¿»è¯‘å™¨ï¼ˆæ›´ä¿å®ˆçš„ç­–ç•¥ï¼‰
    
    ç‰¹ç‚¹ï¼š
    1. åªç¿»è¯‘è¡Œæœ«æ³¨é‡Šï¼ˆ% å¼€å¤´ï¼‰
    2. åªç¿»è¯‘å­—ç¬¦ä¸²ä¸­çš„ä¸­æ–‡ï¼ˆ{å®šä¹‰~}ï¼‰
    3. å®Œå…¨ä¿æŠ¤æ‰€æœ‰ LaTeX å‘½ä»¤
    """
    
    # ğŸ”§ æ›´ä¸¥æ ¼çš„ä¿æŠ¤æ¨¡å¼
    PROTECTED_PATTERNS = [
        # ä¿æŠ¤æ‰€æœ‰ LaTeX å‘½ä»¤ï¼ˆé™¤äº†æ³¨é‡Šï¼‰
        (r'\\[a-zA-Z@]+\*?(?:\[[^\]]*\])*(?:\{[^}]*\})*', 'COMMAND'),
        
        # ä¿æŠ¤æ‰€æœ‰ç¯å¢ƒ
        (r'\\begin\{[^}]+\}.*?\\end\{[^}]+\}', 'ENVIRONMENT'),
        
        # ä¿æŠ¤æ‰€æœ‰é€‰é¡¹
        (r'\[[^\]]*\]', 'OPTION'),
        
        # ä¿æŠ¤æ•°å­—å’Œé•¿åº¦å•ä½
        (r'\d+(?:\.\d+)?(?:pt|bp|cm|mm|em|ex|sp)', 'LENGTH'),
    ]
    
    def extract_translatable_parts(self, content: str) -> List[tuple]:
        """
        æå–å¯ç¿»è¯‘çš„éƒ¨åˆ†ï¼ˆæ³¨é‡Šå’Œä¸­æ–‡å­—ç¬¦ä¸²ï¼‰
        
        è¿”å›: [(start_pos, end_pos, text, type), ...]
        type: 'comment' æˆ– 'chinese_string'
        """
        translatable = []
        
        lines = content.split('\n')
        current_pos = 0
        
        for line_idx, line in enumerate(lines):
            # 1. æå–è¡Œæœ«æ³¨é‡Šï¼ˆ% åé¢çš„ä¸­æ–‡ï¼‰
            comment_match = re.search(r'%\s*(.+)$', line)
            if comment_match:
                comment_text = comment_match.group(1).strip()
                # æ£€æŸ¥æ˜¯å¦åŒ…å«ä¸­æ–‡
                if re.search(r'[\u4e00-\u9fff]', comment_text):
                    start = current_pos + comment_match.start(1)
                    end = current_pos + comment_match.end(1)
                    translatable.append((start, end, comment_text, 'comment'))
            
            # 2. æå–èŠ±æ‹¬å·ä¸­çš„çº¯ä¸­æ–‡å­—ç¬¦ä¸²ï¼ˆå¦‚ {å®šä¹‰~}ï¼‰
            # æ’é™¤å‘½ä»¤å’Œç¯å¢ƒ
            for match in re.finditer(r'\{([^}]+)\}', line):
                text = match.group(1)
                # å¿…é¡»åŒ…å«ä¸­æ–‡ï¼Œä¸”ä¸èƒ½åŒ…å«åæ–œæ ï¼ˆæ’é™¤å‘½ä»¤ï¼‰
                if re.search(r'[\u4e00-\u9fff]', text) and '\\' not in text:
                    start = current_pos + match.start(1)
                    end = current_pos + match.end(1)
                    translatable.append((start, end, text, 'chinese_string'))
            
            current_pos += len(line) + 1  # +1 for newline
        
        return translatable
    
    def split_into_chunks(self, text: str, max_length: int = 1000) -> List[str]:
        """
        æ ·å¼æ–‡ä»¶ä¸“ç”¨åˆ‡åˆ†ï¼ˆæŒ‰æ³¨é‡Šå—ï¼‰
        
        ç­–ç•¥ï¼šæ¯ N æ¡æ³¨é‡Š/ä¸­æ–‡å­—ç¬¦ä¸²ç»„æˆä¸€ä¸ªå—
        """
        chunks = []
        translatable_parts = self.extract_translatable_parts(text)
        
        if not translatable_parts:
            return [text]  # æ— å¯ç¿»è¯‘å†…å®¹ï¼Œè¿”å›åŸæ–‡
        
        # æŒ‰ä½ç½®åˆ†ç»„ï¼ˆæ¯ 20 ä¸ªä¸€ç»„ï¼‰
        chunk_size = 20
        for i in range(0, len(translatable_parts), chunk_size):
            chunk_parts = translatable_parts[i:i+chunk_size]
            
            # æå–è¿™äº›éƒ¨åˆ†çš„æ–‡æœ¬
            chunk_texts = []
            for start, end, txt, typ in chunk_parts:
                chunk_texts.append(f"[{typ}] {txt}")
            
            chunks.append("\n".join(chunk_texts))
        
        return chunks
def find_style_files(tex_file: str, base_dir: str, visited: set = None) -> Dict[str, List[str]]:
    """
    é€’å½’æŸ¥æ‰¾ .cls å’Œ .sty æ–‡ä»¶ï¼ˆé¿å…å¾ªç¯å¼•ç”¨ï¼‰
    """
    if visited is None:
        visited = set()
    
    style_files = {'cls': [], 'sty': []}
    
    # è·å–æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    tex_file_abs = os.path.abspath(tex_file)
    
    # å¦‚æœå·²è®¿é—®è¿‡ï¼Œç›´æ¥è¿”å›
    if tex_file_abs in visited:
        return style_files
    
    # æ ‡è®°ä¸ºå·²è®¿é—®
    visited.add(tex_file_abs)
    
    try:
        with open(tex_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # 1. æŸ¥æ‰¾ \documentclass{xxx}
        for match in re.findall(r'\\documentclass(?:\[.*?\])?\{([^}]+)\}', content):
            cls_file = match.strip()
            if not cls_file.endswith('.cls'):
                cls_file += '.cls'
            
            # å°è¯•åœ¨é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾
            cls_path = os.path.join(base_dir, cls_file)
            if os.path.exists(cls_path):
                cls_path_abs = os.path.abspath(cls_path)
                
                # å¦‚æœè¿™ä¸ª .cls æ–‡ä»¶è¿˜æ²¡è¢«å¤„ç†è¿‡
                if cls_path_abs not in visited:
                    style_files['cls'].append(cls_path_abs)
                    logger.info(f"   Found document class: {cls_file}")
                    
                    # ğŸ†• é€’å½’æŸ¥æ‰¾è¿™ä¸ª .cls æ–‡ä»¶ä¸­å¼•ç”¨çš„æ ·å¼æ–‡ä»¶
                    sub_styles = find_style_files(cls_path, base_dir, visited)
                    style_files['cls'].extend(sub_styles['cls'])
                    style_files['sty'].extend(sub_styles['sty'])
        
        # 2. æŸ¥æ‰¾ \usepackage{xxx}ï¼ˆåªæŸ¥æ‰¾æœ¬åœ°æ–‡ä»¶ï¼‰
        for match in re.findall(r'\\usepackage(?:\[.*?\])?\{([^}]+)\}', content):
            packages = match.split(',')
            for pkg in packages:
                pkg = pkg.strip()
                if not pkg.endswith('.sty'):
                    pkg += '.sty'
                
                # å°è¯•åœ¨é¡¹ç›®ç›®å½•ä¸­æŸ¥æ‰¾
                sty_path = os.path.join(base_dir, pkg)
                if os.path.exists(sty_path):
                    sty_path_abs = os.path.abspath(sty_path)
                    
                    # å¦‚æœè¿™ä¸ª .sty æ–‡ä»¶è¿˜æ²¡è¢«å¤„ç†è¿‡
                    if sty_path_abs not in visited:
                        style_files['sty'].append(sty_path_abs)
                        logger.info(f"   Found package: {pkg}")
                        
                        # ğŸ†• é€’å½’æŸ¥æ‰¾è¿™ä¸ª .sty æ–‡ä»¶ä¸­å¼•ç”¨çš„æ ·å¼æ–‡ä»¶
                        sub_styles = find_style_files(sty_path, base_dir, visited)
                        style_files['cls'].extend(sub_styles['cls'])
                        style_files['sty'].extend(sub_styles['sty'])
        
        # 3. ğŸ†• æŸ¥æ‰¾ \RequirePackage{xxx}ï¼ˆ.cls æ–‡ä»¶å¸¸ç”¨ï¼‰
        for match in re.findall(r'\\RequirePackage(?:\[.*?\])?\{([^}]+)\}', content):
            packages = match.split(',')
            for pkg in packages:
                pkg = pkg.strip()
                if not pkg.endswith('.sty'):
                    pkg += '.sty'
                
                sty_path = os.path.join(base_dir, pkg)
                if os.path.exists(sty_path):
                    sty_path_abs = os.path.abspath(sty_path)
                    
                    if sty_path_abs not in visited:
                        style_files['sty'].append(sty_path_abs)
                        logger.info(f"   Found required package: {pkg}")
                        
                        # é€’å½’æŸ¥æ‰¾
                        sub_styles = find_style_files(sty_path, base_dir, visited)
                        style_files['cls'].extend(sub_styles['cls'])
                        style_files['sty'].extend(sub_styles['sty'])
        
        # 4. ğŸ†• æŸ¥æ‰¾ \input{xxx.sty} æˆ– \input{xxx.cls}ï¼ˆå°‘è§ä½†å¯èƒ½å­˜åœ¨ï¼‰
        for match in re.findall(r'\\input\{([^}]+\.(?:cls|sty))\}', content):
            style_file = match.strip()
            style_path = os.path.join(base_dir, style_file)
            
            if os.path.exists(style_path):
                style_path_abs = os.path.abspath(style_path)
                ext = os.path.splitext(style_file)[1].lower()
                
                if style_path_abs not in visited:
                    if ext == '.cls':
                        style_files['cls'].append(style_path_abs)
                        logger.info(f"   Found input class: {style_file}")
                    elif ext == '.sty':
                        style_files['sty'].append(style_path_abs)
                        logger.info(f"   Found input package: {style_file}")
                    
                    # é€’å½’æŸ¥æ‰¾
                    sub_styles = find_style_files(style_path, base_dir, visited)
                    style_files['cls'].extend(sub_styles['cls'])
                    style_files['sty'].extend(sub_styles['sty'])
    
    except Exception as e:
        logger.error(f"Error reading {tex_file}: {e}")
    
    # å»é‡ï¼ˆä¿æŒé¡ºåºï¼‰
    style_files['cls'] = list(dict.fromkeys(style_files['cls']))
    style_files['sty'] = list(dict.fromkeys(style_files['sty']))
    
    return style_files
def copy_style_files(source_dir: str, dest_dir: str):
    """
    å¤åˆ¶æ ·å¼æ–‡ä»¶ï¼ˆä¸ç¿»è¯‘æ—¶ä½¿ç”¨ï¼‰
    """
    style_extensions = {'.cls', '.sty', '.bst'}
    copied_count = 0
    
    for root, dirs, files in os.walk(source_dir):
        root_abs = os.path.abspath(root)
        dest_dir_abs = os.path.abspath(dest_dir)
        if root_abs == dest_dir_abs or root_abs.startswith(dest_dir_abs + os.sep):
            continue
        
        for file in files:
            _, ext = os.path.splitext(file)
            if ext.lower() in style_extensions:
                source_file = os.path.join(root, file)
                rel_path = os.path.relpath(source_file, source_dir)
                dest_file = os.path.join(dest_dir, rel_path)
                
                try:
                    os.makedirs(os.path.dirname(dest_file), exist_ok=True)
                    shutil.copy2(source_file, dest_file)
                    copied_count += 1
                    logger.info(f"   âœ“ Copied style file: {rel_path}")
                except Exception as e:
                    logger.warning(f"   âš ï¸ Failed to copy {rel_path}: {e}")
    
    if copied_count > 0:
        logger.info(f"   ğŸ“‹ Total style files copied: {copied_count}")
    else:
        logger.info("   â„¹ï¸ No style files to copy")

# ========== ç¼“å­˜ç®¡ç†å·¥å…·å‡½æ•° ==========

def clear_cache(cache_file='translation_cache.json'):
   """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
   cache = TranslationCache(cache_file=cache_file)
   cache.clear_all()
   cache.close()


def clear_old_cache(days: int = 30, cache_file='translation_cache.json'):
   """æ¸…ç†æŒ‡å®šå¤©æ•°å‰çš„ç¼“å­˜"""
   cache = TranslationCache(cache_file=cache_file)
   cache.clear_old(days=days)
   cache.close()


def show_cache_stats(cache_file='translation_cache.json'):
   """æ˜¾ç¤ºç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
   cache = TranslationCache(cache_file=cache_file)
   stats = cache.get_stats()
   
   print("\n" + "=" * 60)
   print("ğŸ“Š Translation Cache Statistics")
   print("=" * 60)
   print(f"Cache file: {cache_file}")
   print(f"Total entries: {stats['cache_entries']}")
   print(f"Cache size: {stats['cache_size_mb']}")
   print(f"Session hits: {stats['hits']}")
   print(f"Session misses: {stats['misses']}")
   print(f"Session hit rate: {stats['hit_rate']}")
   print("=" * 60)
   
   # æ˜¾ç¤ºç¼“å­˜æ¡ç›®çš„æ—¶é—´åˆ†å¸ƒ
   if cache.cache:
       from collections import defaultdict
       age_distribution = defaultdict(int)
       now = datetime.now()
       
       for value in cache.cache.values():
           if isinstance(value, dict) and 'timestamp' in value:
               try:
                   timestamp = datetime.fromisoformat(value['timestamp'])
                   age_days = (now - timestamp).days
                   
                   if age_days == 0:
                       age_distribution['Today'] += 1
                   elif age_days <= 7:
                       age_distribution['This week'] += 1
                   elif age_days <= 30:
                       age_distribution['This month'] += 1
                   elif age_days <= 90:
                       age_distribution['Last 3 months'] += 1
                   else:
                       age_distribution['Older'] += 1
               except:
                   age_distribution['Unknown'] += 1
       
       if age_distribution:
           print("\nğŸ“… Cache age distribution:")
           for period, count in sorted(age_distribution.items()):
               percentage = count / len(cache.cache) * 100
               print(f"   {period}: {count} entries ({percentage:.1f}%)")
           print()
   
   cache.close()
   # 2. æŸ¥çœ‹ç¼“å­˜ç»Ÿè®¡
   # show_cache_stats()
   
   # 3. æ¸…ç† 30 å¤©å‰çš„ç¼“å­˜
   # clear_old_cache(days=30)
   
   # 4. æ¸…ç©ºæ‰€æœ‰ç¼“å­˜
   # clear_cache()
